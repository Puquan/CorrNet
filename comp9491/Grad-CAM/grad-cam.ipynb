{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060bd811-cf86-4f1f-9215-e85185c2d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from utils import video_augmentation\n",
    "from slr_network import SLRModel\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3137d908-e210-4d80-8cdd-8bbcd039b1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.11.0+cu113\n",
      "CUDA version: 11.3\n",
      "cuDNN version: 8200\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f06e09ae-2bdd-4471-a58f-5b20915a2343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e8d0950-58d7-49a4-bcbc-9db935fc8e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating CAM for 11August_2010_Wednesday_tagesschau-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_902/1681878165.py:45: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  video_length = torch.LongTensor([np.ceil(vid.size(1) / total_stride) * total_stride + 2*left_pad ])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 56, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate cam.jpg\n"
     ]
    }
   ],
   "source": [
    "device_id = 0\n",
    "dataset = 'phoenix2014-T'\n",
    "prefix = '/root/autodl-tmp/PHOENIX-2014-T-release-v3/PHOENIX-2014-T'\n",
    "dict_path = f'./preprocess/{dataset}/gloss_dict.npy'\n",
    "model_weights = './_best_model.pt'\n",
    "select_id = 0#0  # The video selected to show\n",
    "#name = '01April_2010_Thursday_heute_default-1'\n",
    "\n",
    "# Load data and apply transformation\n",
    "gloss_dict = np.load(dict_path, allow_pickle=True).item()\n",
    "inputs_list = np.load(f\"./preprocess/{dataset}/dev_info.npy\", allow_pickle=True).item()\n",
    "name = inputs_list[select_id]['fileid']\n",
    "print(f'Generating CAM for {name}')\n",
    "img_folder = os.path.join(prefix, \"features/fullFrame-256x256px/\" + inputs_list[select_id]['folder']) if 'phoenix' in dataset else os.path.join(prefix, \"features/fullFrame-256x256px/\" + inputs_list[select_id]['folder'] + \"/*.jpg\")\n",
    "img_list = sorted(glob.glob(img_folder))\n",
    "img_list = [cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB) for img_path in img_list]\n",
    "label_list = []\n",
    "for phase in inputs_list[select_id]['label'].split(\" \"):\n",
    "    if phase == '':\n",
    "        continue\n",
    "    if phase in gloss_dict.keys():\n",
    "        label_list.append(gloss_dict[phase][0])\n",
    "transform = video_augmentation.Compose([\n",
    "                video_augmentation.CenterCrop(224),\n",
    "                video_augmentation.Resize(1.0),\n",
    "                video_augmentation.ToTensor(),\n",
    "            ])\n",
    "vid, label = transform(img_list, label_list, None)\n",
    "vid = vid.float() / 127.5 - 1\n",
    "vid = vid.unsqueeze(0)\n",
    "\n",
    "left_pad = 0\n",
    "last_stride = 1\n",
    "total_stride = 1\n",
    "kernel_sizes = ['K5', \"P2\", 'K5', \"P2\"]\n",
    "for layer_idx, ks in enumerate(kernel_sizes):\n",
    "    if ks[0] == 'K':\n",
    "        left_pad = left_pad * last_stride \n",
    "        left_pad += int((int(ks[1])-1)/2)\n",
    "    elif ks[0] == 'P':\n",
    "        last_stride = int(ks[1])\n",
    "        total_stride = total_stride * last_stride\n",
    "\n",
    "max_len = vid.size(1)\n",
    "video_length = torch.LongTensor([np.ceil(vid.size(1) / total_stride) * total_stride + 2*left_pad ])\n",
    "right_pad = int(np.ceil(max_len / total_stride)) * total_stride - max_len + left_pad\n",
    "max_len = max_len + left_pad + right_pad\n",
    "vid = torch.cat(\n",
    "    (\n",
    "        vid[0,0][None].expand(left_pad, -1, -1, -1),\n",
    "        vid[0],\n",
    "        vid[0,-1][None].expand(max_len - vid.size(1) - left_pad, -1, -1, -1),\n",
    "    )\n",
    "    , dim=0).unsqueeze(0)\n",
    "\n",
    "fmap_block = list()\n",
    "grad_block = list()\n",
    "\n",
    "device = utils.GpuDataParallel()\n",
    "device.set_device(device_id)\n",
    "# Define model and load state-dict\n",
    "model = SLRModel( num_classes=1116, c2d_type='resnet18', conv_type=2, use_bn=1, gloss_dict=gloss_dict,\n",
    "            loss_weights={'ConvCTC': 1.0, 'SeqCTC': 1.0, 'Dist': 25.0},   )\n",
    "state_dict = torch.load(model_weights)['model_state_dict']\n",
    "state_dict = OrderedDict([(k.replace('.module', ''), v) for k, v in state_dict.items()])\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "model = model.to(device.output_device)\n",
    "model.cuda()\n",
    "\n",
    "model.train()\n",
    "def backward_hook(module, grad_in, grad_out):\n",
    "    grad_block.append(grad_out[0].detach())  #N, C, T, H, W \n",
    "\n",
    "def forward_hook(module, input, output):\n",
    "    fmap_block.append(output)       #N, C, T, H, ,W \n",
    "model.conv2d.layer4[-1].conv1.register_forward_hook(forward_hook)\t\n",
    "model.conv2d.layer4[-1].conv1.register_backward_hook(backward_hook)\n",
    "\n",
    "def cam_show_img(img, feature_map, grads, out_dir):  # img: ntchw, feature_map: ncthw, grads: ncthw\n",
    "    N, C, T, H, W = feature_map.shape\n",
    "    cam = np.zeros(feature_map.shape[2:], dtype=np.float32)\t# thw\n",
    "    grads = grads[0,:].reshape([C, T, -1])\t\t\t\t\t\n",
    "    weights = np.mean(grads, axis=-1)\t\n",
    "    for i in range(C):\t\t\t\t\t\t\n",
    "        for j in range(T):\n",
    "            cam[j] += weights[i,j] * feature_map[0, i, j, :, :]\t\t\n",
    "    cam = np.maximum(cam, 0)\t\t\t\t\t\n",
    "\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    else:\n",
    "        import shutil\n",
    "        shutil.rmtree(out_dir)\n",
    "        os.makedirs(out_dir)\n",
    "    for i in range(T):\n",
    "        out_cam = cam[i]\n",
    "        out_cam = out_cam - np.min(out_cam)\n",
    "        out_cam = out_cam / (1e-7 + out_cam.max())\n",
    "        out_cam = cv2.resize(out_cam, (img.shape[3], img.shape[4]))\n",
    "        out_cam = (255 * out_cam).astype(np.uint8)\n",
    "        heatmap = cv2.applyColorMap(out_cam, cv2.COLORMAP_JET)\n",
    "        cam_img = np.float32(heatmap) / 255 + (img[0,i]/2+0.5).permute(1,2,0).cpu().data.numpy()\n",
    "        cam_img = cam_img/np.max(cam_img)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        path_cam_img = os.path.join(out_dir, f\"cam_{i}.jpg\")\n",
    "        cv2.imwrite(path_cam_img, cam_img)\n",
    "    print('Generate cam.jpg')\n",
    "\n",
    "print(vid.shape)\n",
    "vid = device.data_to_device(vid)\n",
    "vid_lgt = device.data_to_device(video_length)\n",
    "label = device.data_to_device([torch.LongTensor(label)])\n",
    "label_lgt = device.data_to_device(torch.LongTensor([len(label_list)]))\n",
    "ret_dict = model(vid, vid_lgt, label=label, label_lgt=label_lgt)\n",
    "\n",
    "model.zero_grad()\n",
    "for i in range(ret_dict['sequence_logits'].size(0)):\n",
    "    idx = np.argmax(ret_dict['sequence_logits'].cpu().data.numpy()[i,0])  #TBC\n",
    "    class_loss = ret_dict['sequence_logits'][i, 0, idx]\n",
    "    class_loss.backward(retain_graph=True)\n",
    "# generate cam\n",
    "grads_val = torch.stack(grad_block,1).mean(1).cpu().data.numpy()\n",
    "fmap = fmap_block[0].cpu().data.numpy()\n",
    "# save image\n",
    "cam_show_img(vid, fmap, grads_val, out_dir='./weight_map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f1a804-6c60-4e23-9b96-dfee53b3a354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
